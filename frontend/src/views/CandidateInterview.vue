<template>
  <div class="candidate-interview">
    <el-container>
      <el-header>
        <div class="header-content">
          <h1>–°–µ—Å—Å–∏—è –ò–ò-–ò–Ω—Ç–µ—Ä–≤—å—é</h1>
          <div class="session-info" v-if="sessionData">
            <span>–°–µ—Å—Å–∏—è: {{ sessionData.session_id }}</span>
            <span>–°—Ç–∞—Ç—É—Å: {{ getStatusLabel(sessionData.status) }}</span>
          </div>
        </div>
      </el-header>
      
      <el-main>
        <el-row :gutter="20">
          <!-- Avatar Section -->
          <el-col :span="16">
            <el-card class="avatar-card">
              <template #header>
                <span>–ê–≤–∞—Ç–∞—Ä –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞</span>
              </template>
              
              <div class="avatar-container">
                <StreamingAvatarPlayer 
                  ref="avatarPlayerRef"
                  :session-id="sessionId"
                  @avatar-connected="handleAvatarConnected"
                  @avatar-disconnected="handleAvatarDisconnected"
                  @avatar-question="handleAvatarQuestion"
                  @avatar-speak="handleAvatarSpeak"
                />
              </div>
            </el-card>
          </el-col>
          
          <!-- Chat Section -->
          <el-col :span="8">
            <el-card class="chat-card">
              <!-- Code Input Section -->
              <div v-if="!interviewStarted" class="code-input-section">
                <div class="code-input-wrapper">
                  <el-input
                    v-model="interviewCode"
                    placeholder="–í–≤–µ–¥–∏—Ç–µ –∫–æ–¥ –∏–Ω—Ç–µ—Ä–≤—å—é (6 —Ü–∏—Ñ—Ä)"
                    maxlength="6"
                    style="width: 200px; margin-right: 10px;"
                    @keyup.enter="validateCode"
                  >
                    <template #prefix>
                      <el-icon><Key /></el-icon>
                    </template>
                  </el-input>
                  <el-button 
                    type="primary" 
                    @click="validateCode"
                    :loading="validatingCode"
                    style="white-space: pre-line; height: auto; line-height: 1.2;"
                  >
                    –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å<br>–∫–æ–¥
                  </el-button>
                  <el-button 
                    type="success" 
                    @click="startInterview"
                    :disabled="interviewStarted || !resumeLinked"
                    style="white-space: pre-line; height: auto; line-height: 1.2;"
                  >
                    –ù–∞—á–∞—Ç—å<br>–∏–Ω—Ç–µ—Ä–≤—å—é
                  </el-button>
                  <el-button 
                    type="danger" 
                    @click="endInterview"
                    :disabled="!interviewStarted"
                    style="white-space: pre-line; height: auto; line-height: 1.2;"
                  >
                    –ó–∞–≤–µ—Ä—à–∏—Ç—å<br>–∏–Ω—Ç–µ—Ä–≤—å—é
                  </el-button>
                </div>
                <div v-if="codeError" class="code-error">
                  {{ codeError }}
                </div>
              </div>
              
              <!-- Resume Info -->
              <div v-if="resumeLinked && linkedResume" class="resume-info">
                <el-tag type="success" size="large">
                  <el-icon><Document /></el-icon>
                  –†–µ–∑—é–º–µ –ø—Ä–∏–≤—è–∑–∞–Ω–æ: {{ linkedResume.original_filename }}
                </el-tag>
              </div>
              
              <!-- Chat Messages -->
              <div class="chat-messages" ref="chatContainer">
                <div 
                  v-for="message in chatMessages" 
                  :key="message.id"
                  :class="['message', message.type]"
                >
                  <div class="message-content">
                    <div class="message-text">{{ message.text }}</div>
                    <div class="message-time">{{ formatTime(message.timestamp) }}</div>
                  </div>
                </div>
              </div>
              
              <!-- Voice Input -->
              <div class="voice-input-horizontal">
                <!-- Microphone Status -->
                <div class="microphone-status-group">
                  <el-tag 
                    :type="availableMicrophones.length > 0 ? 'success' : 'warning'"
                    size="small"
                  >
                    <span v-html="availableMicrophones.length > 0 ? 'üé§ –ú–∏–∫—Ä–æ—Ñ–æ–Ω<br>–¥–æ—Å—Ç—É–ø–µ–Ω' : '‚ö†Ô∏è –ú–∏–∫—Ä–æ—Ñ–æ–Ω<br>–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'"></span>
                  </el-tag>
                                        <el-button 
                        v-if="availableMicrophones.length === 0"
                        type="warning" 
                        size="small"
                        @click="requestMicrophonePermission"
                        :disabled="!resumeLinked || isVideoPlaying || isWaitingForVideo"
                        style="margin-left: 10px;"
                      >
                        üîß –ó–∞–ø—Ä–æ—Å–∏—Ç—å –¥–æ—Å—Ç—É–ø
                      </el-button>
                </div>
                
                <!-- Microphone Selection -->
                <div class="microphone-selector-group" v-if="availableMicrophones.length > 1">
                  <label>–ú–∏–∫—Ä–æ—Ñ–æ–Ω:</label>
                  <el-select 
                    v-model="selectedMicrophone" 
                    placeholder="–í—ã–±–µ—Ä–∏—Ç–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω"
                    size="small"
                    style="width: 200px;"
                  >
                    <el-option
                      v-for="mic in availableMicrophones"
                      :key="mic.deviceId"
                      :label="mic.label || `–ú–∏–∫—Ä–æ—Ñ–æ–Ω ${mic.deviceId.slice(0, 8)}`"
                      :value="mic.deviceId"
                    />
                  </el-select>
                </div>
                
                <!-- Voice Control Buttons -->
                <div class="voice-controls-group">
                  <!-- Answer Button -->
                  <el-button 
                    type="primary" 
                    size="small"
                    @click="startRecording"
                    :disabled="isRecording || !resumeLinked || isVideoPlaying || isWaitingForVideo"
                    style="flex: 1;"
                  >
                    {{ isRecording ? 'üé§ –ó–∞–ø–∏—Å—å...' : 'üé§ –û—Ç–≤–µ—Ç' }}
                  </el-button>
                  
                  <!-- Stop Button -->
                  <el-button 
                    type="danger" 
                    size="small"
                    @click="stopRecording"
                    :disabled="!isRecording || isVideoPlaying || isWaitingForVideo"
                    style="flex: 1;"
                  >
                    üõë –°—Ç–æ–ø
                  </el-button>
                </div>
              </div>
                
              <div class="recording-status" v-if="isRecording">
                <el-progress 
                  :percentage="recordingProgress" 
                  :show-text="false"
                  :stroke-width="4"
                />
                <span>{{ recordingDuration }}s</span>
              </div>
              

            </el-card>
          </el-col>
        </el-row>
        
        <!-- Interview Progress -->
        <el-card class="progress-card" v-if="sessionData">
          <template #header>
            <span>–ü—Ä–æ–≥—Ä–µ—Å—Å –∏–Ω—Ç–µ—Ä–≤—å—é</span>
          </template>
          
          <el-row :gutter="20">
            <el-col :span="12">
              <div class="progress-item">
                <label>–ó–∞–¥–∞–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤:</label>
                <span v-if="scenarioData">
                  {{ sessionData.current_step }}/{{ scenarioData.total_nodes || 0 }}
                </span>
                <span v-else>
                  {{ sessionData.current_step }}/...
                </span>
              </div>
            </el-col>
            <el-col :span="12">
              <div class="progress-item">
                <label>–°—Ç–∞—Ç—É—Å:</label>
                <el-tag :type="getStatusType(sessionData.status)">
                  {{ getStatusLabel(sessionData.status) }}
                </el-tag>
              </div>
            </el-col>
          </el-row>
        </el-card>
      </el-main>
    </el-container>
  </div>
</template>

<script setup>
import { ref, reactive, onMounted, nextTick, computed } from 'vue'
import { useRoute } from 'vue-router'
import { ElMessage } from 'element-plus'
import { User, VideoCamera, Key, Document } from '@element-plus/icons-vue'
import { uploadAudioToMinio } from '@/utils/minio'
import StreamingAvatarPlayer from '@/components/StreamingAvatarPlayer.vue'

// Reactive data
const sessionData = ref(null)
const avatarUrl = ref('')
const avatarLoading = ref(false)
const interviewStarted = ref(false)
const isRecording = ref(false)
const recordingProgress = ref(0)
const recordingDuration = ref(0)

const chatMessages = ref([])
const chatContainer = ref(null)
const currentEmotion = ref('neutral')
const vadStatus = ref('listening') // 'listening', 'speaking', 'silence'
const availableMicrophones = ref([])
const selectedMicrophone = ref('')
const lastVadChange = ref(0) // For debouncing
const currentQuestion = ref(null)

// Interview code variables
const interviewCode = ref('')
const validatingCode = ref(false)
const codeError = ref('')
const resumeLinked = ref(false)
const linkedResume = ref(null)
const scenarioData = ref(null)

// –°–æ—Å—Ç–æ—è–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ
const isVideoPlaying = ref(false)
const isWaitingForVideo = ref(false)

// Recording state
let recordingInterval = null
let mediaRecorder = null
let audioChunks = []
let audioContext = null
let analyser = null
let microphone = null
let vadInterval = null
let silenceStart = null
const VAD_SILENCE_THRESHOLD = 2000 // 2.0 seconds of silence
const VAD_VOLUME_THRESHOLD = 0.1 // Volume threshold for speech detection
const VAD_DEBOUNCE_TIME = 250 // 500ms debounce

// Computed
const sessionId = ref(null)
const avatarPlayerRef = ref(null)

// Methods
const loadScenarioData = async (resumeId) => {
  try {
    // –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∑—é–º–µ, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ —Å–≤—è–∑–∞–Ω–Ω—É—é –≤–∞–∫–∞–Ω—Å–∏—é
    const resumeResponse = await fetch(`/api/v1/resumes/${resumeId}`)
    if (resumeResponse.ok) {
      const resume = await resumeResponse.json()
      
      // –ü–æ–ª—É—á–∞–µ–º —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏
      if (resume.vacancy_id) {
        const scenarioResponse = await fetch(`/api/v1/scenarios/by-vacancy/${resume.vacancy_id}`)
        if (scenarioResponse.ok) {
          const scenario = await scenarioResponse.json()
          scenarioData.value = scenario
        }
      }
    }
  } catch (error) {
    console.error('Error loading scenario data:', error)
  }
}

const validateCode = async () => {
  if (!interviewCode.value || interviewCode.value.length !== 6) {
    codeError.value = '–ö–æ–¥ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 6 —Ü–∏—Ñ—Ä'
    return
  }
  
  validatingCode.value = true
  codeError.value = ''
  
  try {
    const response = await fetch('/api/v1/interview-codes/validate', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        code: interviewCode.value
      })
    })
    
    const result = await response.json()
    
    if (result.valid) {
      // Load resume data
      const resumeResponse = await fetch(`/api/v1/resumes/${result.resume_id}`)
      if (resumeResponse.ok) {
        linkedResume.value = await resumeResponse.json()
      }
      
      // Load scenario data
      await loadScenarioData(result.resume_id)
      
      resumeLinked.value = true
      ElMessage.success('–ö–æ–¥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω! –†–µ–∑—é–º–µ –ø—Ä–∏–≤—è–∑–∞–Ω–æ –∫ –∏–Ω—Ç–µ—Ä–≤—å—é.')
    } else {
      codeError.value = result.message || '–ù–µ–≤–µ—Ä–Ω—ã–π –∫–æ–¥'
    }
  } catch (error) {
    codeError.value = '–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–¥–∞'
    console.error('Code validation error:', error)
  } finally {
    validatingCode.value = false
  }
}

const loadSessionData = async () => {
  try {
    // In production, this would fetch from API
    sessionData.value = {
      session_id: sessionId.value,
      status: 'in_progress',
      current_step: 0,
      total_steps: 8,
      total_score: 0.0
    }
  } catch (error) {
    ElMessage.error('Failed to load session data')
    console.error('Error loading session data:', error)
  }
}





const startInterview = async () => {
  try {
    // Create session in database with resume link
    const sessionData = {
      vacancy_id: linkedResume.value?.vacancy_id || 'SWE_BACK_001', // Use resume's vacancy or default
      phone: '+7-999-999-99-99', // Default phone
      email: 'candidate@example.com', // Default email
      resume_id: linkedResume.value?.id // Link to resume
    }
    
    const sessionResponse = await fetch('/api/v1/sessions/', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(sessionData)
    })
    
    if (!sessionResponse.ok) {
      throw new Error('Failed to create session')
    }
    
    const sessionResponseData = await sessionResponse.json()
    sessionId.value = sessionResponseData.id
    console.log('Session created with ID:', sessionId.value)
    
    interviewStarted.value = true
    
    // 1. –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —á–∞—Ç
    const welcomeMessage = '–ü—Ä–∏–≤–µ—Ç! –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –Ω–∞ –ò–ò-–∏–Ω—Ç–µ—Ä–≤—å—é. –Ø –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –∑–∞–¥–∞—Ç—å –≤–∞–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –≤–∞—à–µ–º –æ–ø—ã—Ç–µ –∏ –Ω–∞–≤—ã–∫–æ–≤. –ì–æ—Ç–æ–≤—ã –Ω–∞—á–∞—Ç—å?'
    
    addMessage({
      id: `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      type: 'avatar',
      text: welcomeMessage,
      timestamp: new Date()
    })
    
    // 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    await saveMessageToDatabase(
      welcomeMessage,
      'avatar',
      `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
    )
    
    // 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è
    console.log('Generating video for welcome message...')
    disableUserInput() // –ë–ª–æ–∫–∏—Ä—É–µ–º –≤–≤–æ–¥
    await generateAvatarVideo(welcomeMessage)
    
    // 4. –ñ–î–ï–ú –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤–∏–¥–µ–æ
    console.log('Waiting for welcome video to complete...')
    await waitForVideoCompletion()
    
    // 5. –¢–û–õ–¨–ö–û –ü–û–¢–û–ú –ø–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –≤–æ–ø—Ä–æ—Å
    console.log('Welcome video completed, getting first question...')
    await getNextQuestion()
    
    ElMessage.success('–ò–Ω—Ç–µ—Ä–≤—å—é –Ω–∞—á–∞–ª–æ—Å—å')
  } catch (error) {
    ElMessage.error('–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—á–∞—Ç—å –∏–Ω—Ç–µ—Ä–≤—å—é')
    console.error('Error starting interview:', error)
  }
}

const getNextQuestion = async () => {
  try {
    if (!sessionId.value) return
    
    // –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π API endpoint —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    const response = await fetch(`/api/v1/llm-interview/generate-question`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        session_id: sessionId.value,
        vacancy_id: linkedResume.value?.vacancy_id || 'SWE_BACK_001',
        scenario_node_id: currentQuestion.value?.node_id || null,
        previous_answers: chatMessages.value
          .filter(msg => msg.type === 'user')
          .map(msg => msg.text)
      })
    })
    
    if (response.ok) {
      const result = await response.json()
      const questionData = result.question_data
      
      // –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º –≤–æ–ø—Ä–æ—Å–µ
      currentQuestion.value = {
        ...questionData,
        is_contextual: result.is_contextual || false,
        contextual_question_id: questionData.contextual_question_id || null
      }
      
      // 1. –î–æ–±–∞–≤–ª—è–µ–º –≤–æ–ø—Ä–æ—Å –≤ —á–∞—Ç
      addMessage({
        id: `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
        type: 'avatar',
        text: questionData.question_text,
        timestamp: new Date(),
        is_contextual: result.is_contextual || false
      })
      
      // 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ–ø—Ä–æ—Å –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
      await saveMessageToDatabase(
        questionData.question_text,
        'avatar',
        `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
      )
      
      // 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
      console.log('Generating video for question:', questionData.question_text.substring(0, 50) + '...')
      disableUserInput() // –ë–ª–æ–∫–∏—Ä—É–µ–º –≤–≤–æ–¥
      await generateAvatarVideo(questionData.question_text)
      
      // 4. –ñ–î–ï–ú –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤–∏–¥–µ–æ
      console.log('Waiting for question video to complete...')
      await waitForVideoCompletion()
      
      // 5. –¢–µ–ø–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å
      console.log('Question video completed, user can now answer')
      enableUserInput() // –†–∞–∑–±–ª–æ–∫–∏—Ä—É–µ–º –≤–≤–æ–¥
    }
  } catch (error) {
    console.error('Error getting next question:', error)
  }
}

const endInterview = async () => {
  try {
    // Update session status to completed
    if (sessionId.value) {
      await fetch(`/api/v1/sessions/${sessionId.value}`, {
        method: 'PATCH',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          status: 'completed',
          finished_at: new Date().toISOString()
        })
      })
    }
    
    interviewStarted.value = false
    
    addMessage({
      id: `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      type: 'avatar',
      text: '–°–ø–∞—Å–∏–±–æ –∑–∞ —É—á–∞—Å—Ç–∏–µ –≤ –∏–Ω—Ç–µ—Ä–≤—å—é. –í–∞—à–∏ –æ—Ç–≤–µ—Ç—ã –∑–∞–ø–∏—Å–∞–Ω—ã –∏ –±—É–¥—É—Ç –æ—Ü–µ–Ω–µ–Ω—ã. –£–¥–∞—á–∏!',
      timestamp: new Date()
    })
    
    // Save end message to database
    await saveMessageToDatabase(
      '–°–ø–∞—Å–∏–±–æ –∑–∞ —É—á–∞—Å—Ç–∏–µ –≤ –∏–Ω—Ç–µ—Ä–≤—å—é. –í–∞—à–∏ –æ—Ç–≤–µ—Ç—ã –∑–∞–ø–∏—Å–∞–Ω—ã –∏ –±—É–¥—É—Ç –æ—Ü–µ–Ω–µ–Ω—ã. –£–¥–∞—á–∏!',
      'avatar',
      `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
    )
    
    ElMessage.success('–ò–Ω—Ç–µ—Ä–≤—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ')
  } catch (error) {
    ElMessage.error('–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≤–µ—Ä—à–∏—Ç—å –∏–Ω—Ç–µ—Ä–≤—å—é')
    console.error('Error ending interview:', error)
  }
}



const startRecording = async () => {
  try {
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏–Ω—Ç–µ—Ä–≤—å—é –Ω–∞—á–∞–ª–æ—Å—å –∏ –µ—Å—Ç—å session ID
    if (!sessionId.value) {
      ElMessage.warning('–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –Ω–∞—á–∞—Ç—å –∏–Ω—Ç–µ—Ä–≤—å—é')
      return
    }
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å navigator.mediaDevices
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      throw new Error('MediaDevices API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ —ç—Ç–æ–º –±—Ä–∞—É–∑–µ—Ä–µ')
    }
    
    const constraints = {
      audio: {
        deviceId: selectedMicrophone.value ? { exact: selectedMicrophone.value } : undefined
      }
    }
    console.log('Starting recording with microphone:', selectedMicrophone.value)
    const stream = await navigator.mediaDevices.getUserMedia(constraints)
    mediaRecorder = new MediaRecorder(stream)
    audioChunks = []
    
    mediaRecorder.ondataavailable = (event) => {
      audioChunks.push(event.data)
    }
    
    mediaRecorder.onstop = async () => {
      // Use the actual MIME type from MediaRecorder
      const mimeType = mediaRecorder.mimeType || 'audio/webm'
      const audioBlob = new Blob(audioChunks, { type: mimeType })
      console.log('VAD: Audio recorded, MIME type:', mimeType, 'Size:', audioBlob.size)
      await processAudioMessage(audioBlob)
    }
    
    // Initialize VAD
    await initializeVAD(stream)
    
    mediaRecorder.start()
    isRecording.value = true
    recordingDuration.value = 0
    recordingProgress.value = 0
    silenceStart = null
    
    // Start timer
    recordingInterval = setInterval(() => {
      recordingDuration.value++
      recordingProgress.value = Math.min((recordingDuration.value / 60) * 100, 100)
      
      if (recordingDuration.value >= 60) {
        stopRecording()
      }
    }, 1000)
    
    // Start VAD monitoring
    startVADMonitoring()
    
  } catch (error) {
    ElMessage.error('Failed to start recording')
    console.error('Error starting recording:', error)
  }
}

const stopRecording = () => {
  if (mediaRecorder && isRecording.value) {
    mediaRecorder.stop()
    mediaRecorder.stream.getTracks().forEach(track => track.stop())
    isRecording.value = false
    
    if (recordingInterval) {
      clearInterval(recordingInterval)
      recordingInterval = null
    }
    
    // Clean up VAD
    stopVADMonitoring()
  }
}

const processAudioMessage = async (audioBlob) => {
  try {
    // Add user message placeholder
    const messageId = `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
    addMessage({
      id: messageId,
      type: 'user',
      text: 'üé§ [Processing voice message...]',
      timestamp: new Date()
    })
    
    // Generate filename
    const fileName = `recording_${Date.now()}.${audioBlob.type.split('/')[1] || 'webm'}`
    
    console.log('MinIO: Starting audio upload, file:', fileName, 'size:', audioBlob.size)
    
    // Upload audio to MinIO and get transcription in one step
    const formData = new FormData()
    formData.append('audio', audioBlob, fileName)
    formData.append('session_id', sessionId.value)
    
    const sttResponse = await fetch('/api/v1/stt/transcribe-file', {
      method: 'POST',
      body: formData
    })
    
    console.log('VAD: STT response status:', sttResponse.status)
    
    if (!sttResponse.ok) {
      const errorText = await sttResponse.text()
      console.error('VAD: STT service failed:', errorText)
      throw new Error(`STT service failed: ${sttResponse.status} - ${errorText}`)
    }
    
    const sttResult = await sttResponse.json()
    console.log('VAD: STT result:', sttResult)
    const transcribedText = sttResult.text || "Could not transcribe audio"
    
    // Update the message with transcribed text
    const lastMessage = chatMessages.value[chatMessages.value.length - 1]
    lastMessage.text = transcribedText
    
    // Save to database via orchestrator with audio URL
    const audioUrl = sttResult.audio_url || null
    const transcriptionConfidence = sttResult.confidence || null
    await saveMessageToDatabase(transcribedText, 'user', messageId, audioUrl, transcriptionConfidence)
    
    // Analyze tone of voice and update avatar emotion (silently)
    const toneAnalysis = await analyzeToneAndUpdateAvatar(transcribedText)
    
    // Analyze answer and save to QA table if we have current question
    if (currentQuestion.value?.question_text) {
      await analyzeAndSaveAnswer(
        currentQuestion.value.question_text,
        transcribedText,
        audioUrl,
        currentQuestion.value.question_id || 'unknown'
      )
    }
    
    // Get avatar response
    await getAvatarResponse(transcribedText)
    
    // –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –æ—Ç–º–µ—á–∞–µ–º –µ–≥–æ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π
    if (currentQuestion.value?.is_contextual && currentQuestion.value?.contextual_question_id) {
      await markContextualQuestionAsUsed(currentQuestion.value.contextual_question_id)
    }
    
    // Get next question after a short delay
    setTimeout(async () => {
      await getNextQuestion()
    }, 2000)
    
  } catch (error) {
    ElMessage.error('Failed to process audio message')
    console.error('Error processing audio message:', error)
    
    // Update message with error
    const lastMessage = chatMessages.value[chatMessages.value.length - 1]
    lastMessage.text = '‚ùå Failed to process voice message'
  }
}



const analyzeAndSaveAnswer = async (questionText, answerText, audioUrl, questionId) => {
  try {
    console.log('Analyzing and saving answer:', { questionText, answerText, audioUrl, questionId })
    
    const response = await fetch('/api/v1/llm-interview/analyze-answer', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        question_text: questionText,
        answer_text: answerText,
        session_id: sessionId.value,
        audio_url: audioUrl,
        question_id: questionId,
        vacancy_requirements: linkedResume.value?.vacancy_requirements || ''
      })
    })
    
    if (response.ok) {
      const result = await response.json()
      console.log('Answer analysis result:', result)
      
      if (result.qa_record) {
        console.log('QA record saved:', result.qa_record)
      }
    } else {
      console.error('Failed to analyze answer:', response.status, response.statusText)
    }
  } catch (error) {
    console.error('Error analyzing and saving answer:', error)
  }
}

// –ï–¥–∏–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∞–≤–∞—Ç–∞—Ä–∞ (DRY)
const generateAvatarVideo = async (text) => {
  try {
    console.log('Generating avatar video for text:', text.substring(0, 50) + '...')
    
    const avatarResponse = await fetch(`/api/v1/llm-interview/avatar-speak`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        session_id: sessionId.value,
        text: text,
        avatar_id: '68af59a86eeedd0042ca7e27', // Alice (working for video)
        voice_id: '66d3f6a704d077b1432fb7d3'  // Anna
      })
    })
    
    if (avatarResponse.ok) {
      const avatarResult = await avatarResponse.json()
      if (avatarResult.success && avatarResult.mode === 'fallback_video') {
        console.log('Fallback video generated:', avatarResult.video_url)
        avatarPlayerRef.value?.setVideoUrl(avatarResult.video_url)
        return avatarResult.video_url
      } else if (avatarResult.success && avatarResult.mode === 'streaming') {
        console.log('Streaming mode activated')
        return 'streaming'
      }
    }
  } catch (error) {
    console.warn('Avatar video generation failed:', error)
  }
  return null
}

// –ú–µ—Ç–æ–¥—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (KISS)
const enableUserInput = () => {
  isVideoPlaying.value = false
  isWaitingForVideo.value = false
  console.log('User input enabled')
}

const disableUserInput = () => {
  isVideoPlaying.value = true
  isWaitingForVideo.value = true
  console.log('User input disabled')
}

// –ú–µ—Ç–æ–¥ –æ–∂–∏–¥–∞–Ω–∏—è –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤–∏–¥–µ–æ (KISS)
const waitForVideoCompletion = () => {
  return new Promise((resolve) => {
    if (!avatarPlayerRef.value) {
      console.log('No avatar player ref, resolving immediately')
      resolve()
      return
    }
    
    console.log('Waiting for video completion...')
    
    // –°–ª—É—à–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤–∏–¥–µ–æ
    const onVideoEnd = () => {
      console.log('Video ended, resolving promise')
      resolve()
      // –£–±–∏—Ä–∞–µ–º —Å–ª—É—à–∞—Ç–µ–ª—å
      const videoElement = avatarPlayerRef.value?.$el?.querySelector('video')
      if (videoElement) {
        videoElement.removeEventListener('ended', onVideoEnd)
      }
    }
    
    // –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—à–∞—Ç–µ–ª—å
    const videoElement = avatarPlayerRef.value?.$el?.querySelector('video')
    if (videoElement) {
      videoElement.addEventListener('ended', onVideoEnd)
      console.log('Video end listener added')
    } else {
      console.log('Video element not found, resolving in 5 seconds')
    }
    
    // Fallback: –µ—Å–ª–∏ –≤–∏–¥–µ–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, —Ä–∞–∑—Ä–µ—à–∞–µ–º —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥
    setTimeout(() => {
      console.log('Video completion timeout, resolving')
      resolve()
    }, 5000)
  })
}

const getAvatarResponse = async (userMessage) => {
  try {
    // Check if session ID is available
    if (!sessionId.value) {
      console.error('No session ID available, cannot get avatar response')
      addMessage({
        id: `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
        type: 'avatar',
        text: '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.',
        timestamp: new Date()
      })
      return
    }

    // Step 1: Get LLM response
    const llmResponse = await fetch(`/api/v1/llm/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        session_id: sessionId.value,
        message: userMessage
      })
    })
    
    if (llmResponse.ok) {
      const llmResult = await llmResponse.json()
      const avatarText = llmResult.response
      
      // 1. –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ —á–∞—Ç
      addMessage({
        id: `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
        type: 'avatar',
        text: avatarText,
        timestamp: new Date()
      })
      
      // 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
      console.log('Generating video for LLM response:', avatarText.substring(0, 50) + '...')
      disableUserInput() // –ë–ª–æ–∫–∏—Ä—É–µ–º –≤–≤–æ–¥
      await generateAvatarVideo(avatarText)
      
      // 3. –ñ–î–ï–ú –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤–∏–¥–µ–æ
      console.log('Waiting for response video to complete...')
      await waitForVideoCompletion()
      
      // 4. –¢–û–õ–¨–ö–û –ü–û–¢–û–ú –ø–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å
      console.log('Response video completed, getting next question...')
      await getNextQuestion()
      
    } else {
      // Fallback response
      addMessage({
        id: `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
        type: 'avatar',
        text: '–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à –æ—Ç–≤–µ—Ç. –ú–æ–∂–µ—Ç–µ –ª–∏ –≤—ã —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –æ–± —ç—Ç–æ–º –ø–æ–¥—Ä–æ–±–Ω–µ–µ?',
        timestamp: new Date()
      })
    }
  } catch (error) {
    ElMessage.error('Failed to get avatar response')
    console.error('Error getting avatar response:', error)
  }
}

const addMessage = (message) => {
  chatMessages.value.push(message)
  scrollToBottom()
}

const scrollToBottom = async () => {
  await nextTick()
  if (chatContainer.value) {
    chatContainer.value.scrollTop = chatContainer.value.scrollHeight
  }
}

const formatTime = (timestamp) => {
  return new Date(timestamp).toLocaleTimeString()
}

const getStatusType = (status) => {
  const types = {
    created: 'info',
    in_progress: 'warning',
    completed: 'success',
    failed: 'danger'
  }
  return types[status] || 'info'
}

const getStatusLabel = (status) => {
  const labels = {
    created: '–°–æ–∑–¥–∞–Ω–∞',
    in_progress: '–í –ø—Ä–æ—Ü–µ—Å—Å–µ',
    completed: '–ó–∞–≤–µ—Ä—à–µ–Ω–∞',
    failed: '–û—à–∏–±–∫–∞'
  }
  return labels[status] || status
}

const analyzeToneAndUpdateAvatar = async (text) => {
  try {
    // Check if session ID is available
    if (!sessionId.value) {
      console.error('No session ID available, cannot analyze tone')
      currentEmotion.value = 'neutral'
      return 'neutral'
    }

    // Silently analyze tone for internal avatar emotion updates
    const response = await fetch(`/api/v1/llm/analyze-tone`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        text: text,
        session_id: sessionId.value
      })
    })
    
    if (response.ok) {
      const result = await response.json()
      const detectedEmotion = result.emotion || 'neutral'
      
      // Update internal emotion state (not visible to candidate)
      currentEmotion.value = detectedEmotion
      
      // Return the tone analysis result for saving to database
      return detectedEmotion
    }
  } catch (error) {
    console.error('Error analyzing tone:', error)
    // Fallback to neutral emotion
    currentEmotion.value = 'neutral'
    return 'neutral'
  }
}

const initializeVAD = async (stream) => {
  try {
    console.log('VAD: Initializing...')
    audioContext = new (window.AudioContext || window.webkitAudioContext)()
    analyser = audioContext.createAnalyser()
    microphone = audioContext.createMediaStreamSource(stream)
    
    analyser.fftSize = 256
    analyser.smoothingTimeConstant = 0.8
    
    microphone.connect(analyser)
    console.log('VAD: Initialized successfully')
  } catch (error) {
    console.error('VAD: Failed to initialize:', error)
  }
}

const startVADMonitoring = () => {
  if (!analyser) {
    console.log('VAD: Analyser not available')
    return
  }
  
  console.log('VAD: Starting monitoring')
  vadStatus.value = 'listening'
  
  vadInterval = setInterval(() => {
    try {
      const dataArray = new Uint8Array(analyser.frequencyBinCount)
      analyser.getByteFrequencyData(dataArray)
      
      // Calculate average volume
      const average = dataArray.reduce((a, b) => a + b) / dataArray.length
      const volume = average / 255
      
      // Only log every 50th measurement to reduce console spam
      if (Math.random() < 0.02) {
        console.log('VAD: Volume:', volume.toFixed(4), 'Threshold:', VAD_VOLUME_THRESHOLD)
      }
      
      const now = Date.now()
      
      if (volume > VAD_VOLUME_THRESHOLD) {
        // Speech detected, reset silence timer
        if (vadStatus.value !== 'speaking' && (now - lastVadChange.value) > VAD_DEBOUNCE_TIME) {
          console.log('VAD: Speech detected')
          vadStatus.value = 'speaking'
          lastVadChange.value = now
        }
        silenceStart = null
      } else {
        // Silence detected
        if (silenceStart === null) {
          silenceStart = Date.now()
          if (vadStatus.value !== 'silence' && (now - lastVadChange.value) > VAD_DEBOUNCE_TIME) {
            console.log('VAD: Silence started')
            vadStatus.value = 'silence'
            lastVadChange.value = now
          }
        } else {
          const silenceDuration = Date.now() - silenceStart
          if (silenceDuration > VAD_SILENCE_THRESHOLD) {
            // Stop recording after silence threshold
            console.log('VAD: Silence threshold reached, stopping recording')
            stopRecording()
          }
        }
      }
    } catch (error) {
      console.error('VAD: Error in monitoring:', error)
    }
  }, 100) // Check every 100ms
}

const stopVADMonitoring = () => {
  if (vadInterval) {
    clearInterval(vadInterval)
    vadInterval = null
  }
  
  if (audioContext) {
    audioContext.close()
    audioContext = null
  }
  
  analyser = null
  microphone = null
  silenceStart = null
  vadStatus.value = 'listening'
}

const saveMessageToDatabase = async (text, type, messageId, audioUrl = null, transcriptionConfidence = null, toneAnalysis = null) => {
  try {
    if (!sessionId.value) {
      console.error('No session ID available, cannot save message')
      return
    }
    
    console.log('Saving message to database:', {
      session_id: sessionId.value,
      message_id: messageId,
      text: text,
      message_type: type,
      audio_url: audioUrl,
      transcription_confidence: transcriptionConfidence,
      tone_analysis: toneAnalysis
    })
    
    const response = await fetch('/api/v1/sessions/messages', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
              body: JSON.stringify({
          session_id: sessionId.value,
          message_id: messageId,
          text: text,
          message_type: type,
          timestamp: new Date().toISOString(),
          audio_url: audioUrl,
          transcription_confidence: transcriptionConfidence,
          tone_analysis: toneAnalysis
        })
    })
    
    if (!response.ok) {
      const errorText = await response.text()
      console.error('Failed to save message to database:', response.status, errorText)
    } else {
      console.log('Message saved successfully')
    }
  } catch (error) {
    console.error('Error saving message to database:', error)
  }
}

const getEmotionLabel = (emotion) => {
  const labels = {
    positive: 'üòä Positive',
    neutral: 'üòê Neutral',
    concerned: 'üòü Concerned',
    excited: 'ü§© Excited',
    confused: 'üòï Confused',
    confident: 'üòé Confident'
  }
  return labels[emotion] || 'üòê Neutral'
}

// Lifecycle
onMounted(async () => {
  await loadSessionData()
  
  // –ü—Ä–æ–≤–µ—Ä—è–µ–º HTTPS –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É
  if (location.protocol !== 'https:' && location.hostname !== 'localhost' && location.hostname !== '127.0.0.1') {
    ElMessage.warning('–î–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å HTTPS –∏–ª–∏ localhost')
    console.warn('MediaDevices API —Ç—Ä–µ–±—É–µ—Ç HTTPS (–∫—Ä–æ–º–µ localhost)')
  }
  
  await getAvailableMicrophones()
})

const getAvailableMicrophones = async () => {
  try {
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å navigator.mediaDevices
    if (!navigator.mediaDevices || !navigator.mediaDevices.enumerateDevices) {
      throw new Error('MediaDevices API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ —ç—Ç–æ–º –±—Ä–∞—É–∑–µ—Ä–µ')
    }
    
    // –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤
    try {
      await navigator.mediaDevices.getUserMedia({ audio: true })
    } catch (permissionError) {
      console.warn('–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –¥–∞–ª —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É:', permissionError)
      ElMessage.warning('–î–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–æ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–∞—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –¥–æ—Å—Ç—É–ø –∫ –∞—É–¥–∏–æ')
      return
    }
    
    const devices = await navigator.mediaDevices.enumerateDevices()
    const audioInputs = devices.filter(device => device.kind === 'audioinput')
    availableMicrophones.value = audioInputs
    if (audioInputs.length > 0) {
      selectedMicrophone.value = audioInputs[0].deviceId
    }
    console.log('Available microphones:', audioInputs)
  } catch (error) {
    console.error('Error getting microphones:', error)
    ElMessage.error(`–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É: ${error.message}`)
  }
}

const requestMicrophonePermission = async () => {
  try {
    ElMessage.info('–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É...')
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å navigator.mediaDevices
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      throw new Error('MediaDevices API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ —ç—Ç–æ–º –±—Ä–∞—É–∑–µ—Ä–µ')
    }
    
    // –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
    
    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ç–æ–∫, —Ç–∞–∫ –∫–∞–∫ –Ω–∞–º –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
    stream.getTracks().forEach(track => track.stop())
    
    ElMessage.success('–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω –ø–æ–ª—É—á–µ–Ω–æ!')
    
    // –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω–æ–≤
    await getAvailableMicrophones()
    
  } catch (error) {
    console.error('Error requesting microphone permission:', error)
    
    if (error.name === 'NotAllowedError') {
      ElMessage.error('–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Ä–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.')
    } else if (error.name === 'NotFoundError') {
      ElMessage.error('–ú–∏–∫—Ä–æ—Ñ–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω –ø–æ–¥–∫–ª—é—á–µ–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç.')
    } else {
      ElMessage.error(`–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É: ${error.message}`)
    }
  }
}

// –ú–µ—Ç–æ–¥ –¥–ª—è –æ—Ç–º–µ—Ç–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–≥–æ
const markContextualQuestionAsUsed = async (questionId) => {
  try {
    const response = await fetch(`/api/v1/llm-interview/contextual-questions/${questionId}/mark-used`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        session_id: sessionId.value
      })
    })
    
    if (!response.ok) {
      console.error('Failed to mark contextual question as used:', response.status)
    }
  } catch (error) {
    console.error('Error marking contextual question as used:', error)
  }
}

// Avatar event handlers
const handleAvatarConnected = (data) => {
  console.log('Avatar connected:', data)
  ElMessage.success('–ê–≤–∞—Ç–∞—Ä –ø–æ–¥–∫–ª—é—á–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –æ–±—â–µ–Ω–∏—é!')
}

const handleAvatarDisconnected = (data) => {
  console.log('Avatar disconnected:', data)
  ElMessage.info('–ê–≤–∞—Ç–∞—Ä –æ—Ç–∫–ª—é—á–µ–Ω')
}

const handleAvatarQuestion = (data) => {
  console.log('Avatar question:', data)
  // –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∞–≤–∞—Ç–∞—Ä–∞
  // –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–æ–±–∞–≤–∏—Ç—å –≤ —á–∞—Ç –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç
}

const handleAvatarSpeak = (data) => {
  console.log('Avatar speak:', data)
  // –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏ –∞–≤–∞—Ç–∞—Ä–∞
  // –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–æ–±–∞–≤–∏—Ç—å –≤ —á–∞—Ç –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–∏–∑–Ω–µ—Å–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
}


</script>

<style scoped>
.candidate-interview {
  height: 100vh;
}

.header-content {
  display: flex;
  justify-content: space-between;
  align-items: center;
  height: 100%;
}

.header-content h1 {
  margin: 0;
  color: #303133;
}

.session-info {
  display: flex;
  gap: 20px;
  font-size: 0.9rem;
  color: #606266;
}

.avatar-card {
  height: 600px;
}



.avatar-container {
  display: flex;
  justify-content: center;
  align-items: stretch;
  height: 100%;
  margin-bottom: 20px;
}

.avatar-image {
  width: 200px;
  height: 200px;
  border-radius: 50%;
  object-fit: cover;
  border: 3px solid #409eff;
}

.avatar-placeholder {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 10px;
  color: #909399;
  font-size: 0.9rem;
}

.avatar-placeholder .el-icon {
  font-size: 3rem;
}

.rtsp-placeholder {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  gap: 15px;
  color: #606266;
  font-size: 1rem;
  text-align: center;
  padding: 40px 20px;
  border: 2px dashed #dcdfe6;
  border-radius: 8px;
  background-color: #fafafa;
  width: 100%;
  height: 100%;
  min-height: 300px;
}

.rtsp-placeholder .el-icon {
  font-size: 4rem;
  color: #409eff;
}

.rtsp-placeholder span {
  font-weight: 600;
  color: #303133;
}

.rtsp-placeholder small {
  color: #909399;
  font-size: 0.85rem;
}

.avatar-controls {
  display: flex;
  flex-direction: column;
  gap: 10px;
}

.chat-card {
  height: 600px;
}

.chat-controls {
  display: flex;
  gap: 10px;
  align-items: center;
}

.interview-controls {
  display: flex;
  gap: 10px;
  margin: 15px 0;
  justify-content: flex-start;
  align-items: center;
}

.voice-input-horizontal {
  display: flex;
  justify-content: space-between;
  align-items: center;
  gap: 20px;
  margin: 15px 0;
}

.microphone-status-group {
  display: flex;
  align-items: center;
}

.microphone-selector-group {
  display: flex;
  flex-direction: column;
  align-items: flex-start;
  gap: 5px;
}

.voice-controls-group {
  display: flex;
  gap: 5px;
}

.chat-messages {
  height: 300px;
  overflow-y: auto;
  padding: 10px;
  border: 1px solid #ebeef5;
  border-radius: 4px;
  margin-bottom: 20px;
}

.message {
  margin-bottom: 15px;
  display: flex;
}

.message.user {
  justify-content: flex-end;
}

.message.avatar {
  justify-content: flex-start;
}

.message-content {
  max-width: 70%;
  padding: 10px 15px;
  border-radius: 15px;
  position: relative;
}

.message.user .message-content {
  background-color: #409eff;
  color: white;
}

.message.avatar .message-content {
  background-color: #f5f7fa;
  color: #303133;
}

.message-time {
  font-size: 0.8rem;
  opacity: 0.7;
  margin-top: 5px;
}

.voice-input {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 10px;
  margin-bottom: 20px;
}

.stop-button {
  margin-top: 5px;
  font-weight: bold;
}



.recording-status {
  display: flex;
  align-items: center;
  gap: 10px;
  width: 200px;
}

.text-input {
  margin-top: 10px;
}

.progress-card {
  margin-top: 20px;
}

.progress-item {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 10px;
}

.progress-item label {
  font-weight: 600;
  color: #606266;
}

.vacancy-info-card {
  height: 600px;
}

.vacancy-info {
  height: 100%;
  overflow-y: auto;
}

.vacancy-title {
  display: flex;
  justify-content: space-between;
  align-items: flex-start;
  margin-bottom: 16px;
  padding-bottom: 12px;
  border-bottom: 1px solid #ebeef5;
}

.vacancy-title h4 {
  margin: 0;
  color: #303133;
  font-size: 16px;
  line-height: 1.4;
}

.vacancy-details {
  font-size: 14px;
  color: #606266;
}

.detail-item {
  margin-bottom: 12px;
}

.code-input-section {
  margin-bottom: 15px;
  padding: 15px;
  background-color: #f8f9fa;
  border-radius: 8px;
  border: 1px solid #e9ecef;
}

.code-input-wrapper {
  display: flex;
  align-items: center;
  gap: 10px;
}

.code-error {
  color: #f56c6c;
  font-size: 14px;
  margin-top: 8px;
}

.resume-info {
  margin-bottom: 15px;
}

.resume-info .el-tag {
  font-size: 14px;
  padding: 8px 12px;
}

.detail-item:last-child {
  margin-bottom: 0;
}

.detail-item strong {
  color: #303133;
  display: block;
  margin-bottom: 4px;
}

.detail-item p {
  margin: 0;
  line-height: 1.4;
  color: #606266;
}
</style>
